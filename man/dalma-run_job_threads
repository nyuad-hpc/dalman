.\" generated with Ronn/v0.7.3
.\" http://github.com/rtomayko/ronn/tree/0.7.3
.
.TH "DALMA\-RUN_JOB_THREADS" "" "February 2017" "NYUAD" "Dalma Manual"
.
.SH "SYNOPSIS"
You want to run a job with multi\-threading (OpenMP, Pthreads and etc) application, but without multi\-processing (e\.g\., MPI)\.
.
.SH "DESCRIPTION"
To accomplish this, follow the steps:
.
.P
1\.\- Specify the resouces required in Slurm directives in job script\. Most importantly, \fB\-\-cpus\-per\-task\fR for cpus per tasks\. This should equal to the number of OpenMP threads (\fBOMP_NUM_THREADS\fR) to fully ultilized the allocated resources\.
.
.P
2\.\- Submit the job\.
.
.SH "EXAMPLE"
\fBJob script for an OpenMP job with 28 threads, fully ultilizing 1 compute node\.\fR \fBSave it using a meaningful filename\. E\.g\., "openmp_job\.sh"\fR
.
.IP "" 4
.
.nf

#!/bin/bash
## Set number of nodes to run
#SBATCH \-\-nodes=1
# Set number of tasks to run
#SBATCH \-\-ntasks=1
# Set number of cores per task (default is 1)
#SBATCH \-\-cpus\-per\-task=28
# Walltime format hh:mm:ss
#SBATCH \-\-time=00:30:00
# You may want to be exclusive on the compute node\.
# Remove the extra # below if needed
##SBATCH \-\-exclusive
# Output and error files
#SBATCH \-o job\.%J\.out
#SBATCH \-e job\.%J\.err

# **** Actual commands start here ****
# Set number of OMP threads, directly from SLURM,
#   ultilizting all the cpus allocated\.
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# Load modules here (safety measure)
module purge

# You may need to load gcc here \.\. This is application specific
# module load gcc

# Replace it with your actual command\.
omp\-hello
.
.fi
.
.IP "" 0
.
.P
\fBSubmit the job, run this command in the terminal\fR
.
.IP "" 4
.
.nf

sbatch openmp_job\.sh
.
.fi
.
.IP "" 0
.
.P
At this point, a job is submitted\. A job ID will appears on the screen\. Mark it down for future reference\.
.
.SS "MORE ON SLURM DIRECTIVES"
Normally, all lines begin with \fB#\fR is commented out in bash script\. However, all SLURM directives start with \fB#SBATCH\fR\. To comment out a SLURM directive, use \fB##SBATCH\fR to begin the line\. E\.g\.,
.
.IP "" 4
.
.nf

# The line below is effective
#SBATCH \-\-nodes=1
# The line below is commented out, ineffective
##SBATCH \-\-nodes=1
# The line below is also commented out, as it starts with # but not with #SBATCH
# SBATCH \-\-nodes=1
.
.fi
.
.IP "" 0
.
.P
\fBReceive email notification\fR
.
.IP "" 4
.
.nf

# This line specifies the email address\.
# Change it to your actual email address\.
#SBATCH \-\-mail\-user=first\.last@nyu\.edu
#
# This line specifies when you want to be alerted\.
# If you set it to ALL, you will receive email notifications
# when the job starts, ends, aborts, resubmits\.
# Another useful option is END\. In this case you will only
# receive notification when the job ends\.
#SBATCH \-\-mail\-type=ALL
.
.fi
.
.IP "" 0
.
.P
\fBSpecify memory for the job\fR
.
.IP "" 4
.
.nf

# This directive set the memory required by the job per node
# This example set the memory to 10GB per node
#SBATCH \-\-mem=10G
.
.fi
.
.IP "" 0
.
.SH "AUTHORS"
.
.nf

NYUAD HPC Apps Team:
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
    \- Benoit Marchand
    \- Guowei He
    \- Jorge Naranjo
.
.fi
.
.SH "SEE ALSO"
Please refer to the online documentation available here \fIhttps://nyuad\-hpc\.github\.io/dalman/html/dalma\.html\fR
